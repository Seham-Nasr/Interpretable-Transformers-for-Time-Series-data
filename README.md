# Interpretable-Transformers-for-Time-Series-data
A collection of resources to study Interpretable Transformers for Time Series data

# Tutorials, lectures, and talks
A good starting point is to study some introductory tutorials, lectures, and talks

- [Transformers: The best idea in AI](https://youtu.be/9uw3F6rndnA?t=479)
- [Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://youtu.be/zxQyTK8quyY?t=2](https://youtu.be/zxQyTK8quyY?t=2)https://youtu.be/zxQyTK8quyY?t=2)
- [What are Transformers (Machine Learning Model)?](https://www.youtube.com/watch?v=ZXiruGOCn9s&pp=ygUndHJhbnNmb3JtZXJzIG1hY2hpbmUgbGVhcm5pbmcgZXhwbGFpbmVk)
- [Transformer models and BERT model: Overview](https://youtu.be/t45S_MwAcOw?t=35)
- [Temporal Fusion Transformer](https://medium.com/dataness-ai/understanding-temporal-fusion-transformer-9a7a4fcde74b)
  

# Blogs and post
- [interpretable transformer](https://towardsdatascience.com/tft-an-interpretable-transformer-70147bcf6212)
- [Temporal Fusion Transformers](https://research.google/pubs/temporal-fusion-transformers-for-interpretable-multi-horizon-time-series-forecasting/)
- [How to Apply Transformers to Time Series Models](https://medium.com/intel-tech/how-to-apply-transformers-to-time-series-models-spacetimeformer-e452f2825d2e)
 
# Literature Review papers
- [Transformers in Time-Series Analysis: A Tutorial](https://arxiv.org/pdf/2205.01138.pdf)
- [Long-Range Transformers for Dynamic Spatiotemporal Forecasting](https://arxiv.org/pdf/2109.12218.pdf)
- [Interpreting Time Series Transformer Models and Sensitivity Analysis](https://arxiv.org/pdf/2401.15119.pdf)
- [Time2Vec](https://arxiv.org/pdf/1907.05321.pdf)
- [Self-Attention Attribution](https://arxiv.org/pdf/2004.11207.pdf)* NLP
- [Transformers in Time Series: A Survey](https://arxiv.org/pdf/2202.07125.pdf)

# Interpretable transformers for time series data: papers
- [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)
- [Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency](https://arxiv.org/pdf/2306.02109.pdf)
- [Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks](https://arxiv.org/pdf/2308.05110.pdf)
- [BasisFormer](https://openreview.net/pdf?id=xx3qRKvG0T) <!--Nips2024-->
- [Global TimeSeries Coherence Matrices](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564126)
- [AntroPy](https://ceur-ws.org/Vol-2993/paper-20.pdf)
- [Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting](https://link.springer.com/chapter/10.1007/978-3-031-50396-2_14)
- [Explainable Multivariate Deep Transformer for Stock Prediction](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4713443)
- [Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder](https://arxiv.org/pdf/2105.09428.pdf)
-<!-- [transformers explainability](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf) -->
- [localAndGlobalJournal: Extracting Interpretable Local and Global Representations from Attention on Time Series](https://ui.adsabs.harvard.edu/abs/2023arXiv231211466S/abstract)

### Transformer Attention Mechanism for TS
- [Channel-Wise Attention](https://arxiv.org/pdf/2402.10198v2.pdf)
- [DBAFormer](https://link.springer.com/content/pdf/10.1007/s44230-023-00037-z.pdf)
- [TACTiS](https://proceedings.mlr.press/v162/drouin22a/drouin22a.pdf)
- [Graph Attention Transformer](https://ceur-ws.org/Vol-3343/paper3.pdf)
- [InParformer](https://ojs.aaai.org/index.php/AAAI/article/view/25845)
- [Bidformer](https://ieeexplore.ieee.org/document/10394310)
- [CORRELATED ATTENTION](https://arxiv.org/pdf/2311.11959.pdf)
- [Recurrence and Self-Attention](https://userweb.cs.txstate.edu/~amk181/AIME_LSTM_Attention_vs_Transformer.pdf)
- [Differential Attention](https://arxiv.org/ftp/arxiv/papers/2202/2202.11402.pdf)
- [Attention Augmented](https://arxiv.org/ftp/arxiv/papers/2202/2202.11402.pdf)
- [Abstracting Local Transformer Attention](https://ceur-ws.org/Vol-2993/paper-20.pdf)
- [Show me what you're looking for](https://scholar.google.com/scholar?q=Show+Me+What+You%27re+Looking+For+Visualizing+Abstracted+Transformer+Attention+for+Enhancing+Their+Local+Interpretability+on+Time+Series+Data.)

  
# Github repositories
- [perturbation-based interpretation method^](https://github.com/UVA-MLSys/COVID-19-age-groups) <!-- This work doesn't have new methods, it's good as a reference or reference for the baseline experiments-->
- [ATTATTR](https://github.com/YRdddream/attattr) *NLP Task <!--related to our research question of time series-->
- [Time2Vec^](https://github.com/ojus1/Time2Vec-PyTorch)
- [BasisFormer ^](https://github.com/nzl5116190/Basisformer) <!--the implementation is working fine -->
- [Global TimeSeries Coherence Matrices](https://github.com/cslab-hub/GlobalTimeSeriesCoherenceMatrices)
- [AntroPy](https://github.com/raphaelvallat/antropy)
- [Temporal Saliency Detection ^](https://github.com/duongtrung/time-series-temporal-saliency-patterns/tree/main) <!-- implemented on illness data -->
- <!-- [transformers explainability](https://github.com/hila-chefer/Transformer-Explainability)-->
- [localAndGlobalJournal](https://github.com/cslab-hub/localAndGlobalJournal)

  
- [Channel-Wise Attention](https://github.com/romilbert/samformer)
- [TACTiS](https://github.com/servicenow/tactis)^ <!--Data Augmentation: Copulas might be used to generate synthetic data points that preserve the dependency structure of the original data, which can then be used for data augmentation in tasks like sequence generation or anomaly detection.-->
- [Recurrence and Self-Attention](https://github.com/imics-lab/recurrence-with-self-attention)
- [Samformer](https://github.com/romilbert/samformer)^
- [TSAT](https://github.com/RadiantResearch/TSAT)
  


